{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian density function of $m$-dimensional vectors is:    \n",
    "$g(x;\\mu,C) = {1\\over (\\sqrt{2\\pi})^m |C|^{1/2}} e^{-{1 \\over 2} (x-\\mu)^TC^{-1}(x-\\mu)}$    \n",
    "where $\\mu$ is the distribution mean, $C$ is the covaraince matrix. $|C|$ is the determinant of the matrix $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\mu$ and $C$ can be estimated from the data.\n",
    "$\\mu = {\\sum_{i=1}^n x_i \\over m }$, \n",
    "$C = {\\sum_{i=1}^n (x_i-\\mu)(x_i-\\mu)^T \\over m-1 }$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminant function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $g(x;\\mu_1,C_1)P(h_1) > g(x;\\mu_2,C_2)P(h_2)$, then $x$ is classified as $C_1$.      \n",
    "Problem: there may be no determinant of matrix $C$.     \n",
    "Solution: $ (x-\\mu_1)^TC_1^{-1}(x-\\mu_1) + b < (x-\\mu_2)^TC_2^{-1}(x-\\mu_2)$, where $b$ is a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "\n",
    "import tool\n",
    "\n",
    "class NaiveClassifier:\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef __prior(self):\n",
    "\t\t'''\n",
    "\t\tCalculate the probability for each class.\n",
    "\t\t@information used: self.y, self.n\n",
    "\t\t@ouput:self.priors\n",
    "\t\t'''\n",
    "\n",
    "\t\tself.priors = {}\n",
    "\t\tcounts = self.y.value_counts().to_dict()\n",
    "\t\tfor k, v in counts.items():\n",
    "\t\t\tself.priors[k] = v / self.y.size\n",
    "\n",
    "\tdef __mean_variance(self):\n",
    "\t\t'''\n",
    "\t\tCalculate the mean, variance and so on for each class\n",
    "\t\t'''\n",
    "\n",
    "\t\tself.mean = {}\n",
    "\t\tself.variance = {}\n",
    "\t\tself.determinant = {}\n",
    "\n",
    "\t\tfor c in self.y.unique():\n",
    "\t\t\tidxes = self.y==c\n",
    "\t\t\tX = self.X[idxes,:]\n",
    "\n",
    "\t\t\t# mean\n",
    "\t\t\tmu = np.mean(X,0).reshape((-1,1))\n",
    "\t\t\tself.mean[c] = mu\n",
    "            \n",
    "            # covariance\n",
    "\t\t\tXc = X-mu.T\n",
    "\t\t\tn,m = Xc.shape\n",
    "\t\t\t# var = np.cov(Xc.T)\n",
    "\t\t\tvar = (Xc.T@Xc)/(n-1)\n",
    "\t\t\tself.variance[c] = var\n",
    "            \n",
    "            # determinant\n",
    "\t\t\tself.determinant[c] = np.linalg.det(var)\n",
    "\t\t\t# deal with Singular matrix\n",
    "\t\t\tif np.linalg.det(var) <= 0:\n",
    "\t\t\t\t# tool.printred('nonpositive determinant!!! ' + str(np.linalg.det(var)))\n",
    "\t\t\t\trank = np.linalg.matrix_rank(var)\n",
    "\t\t\t\tD, V = tool.EVD(var)\n",
    "\t\t\t\tD = D[:rank]\n",
    "\t\t\t\tdeterminant = 1\n",
    "\t\t\t\tfor d in D:\n",
    "\t\t\t\t\tdeterminant = determinant*d\n",
    "\t\t\t\tself.determinant[c] = determinant\n",
    "\n",
    "\tdef __calculate_Gaussian_probability(self, x, c):\n",
    "\t\t'''\n",
    "\t\tx: the test data point\n",
    "\t\tc: class\n",
    "\t\t'''\n",
    "\t\tu = self.mean[c]\n",
    "\t\tC = self.variance[c]\n",
    "\t\tdeterminant = self.determinant[c]\n",
    "\t\t\n",
    "\t\tx = x.reshape((-1,1))\n",
    "\n",
    "\t\tm = x.shape[0]\n",
    "\t\tpart1 = ((math.sqrt(2*math.pi))**m)*(determinant**0.5)\n",
    "\t\tif part1 != 0:\n",
    "\t\t    part1 = 1/part1 # pay attention\n",
    "        \n",
    "\t\tmd = (x-u).T@np.linalg.pinv(C)@(x-u)\n",
    "\n",
    "\t\tpart2 = (-1/2)*md\n",
    "\t\tpart2 = math.e**part2\n",
    "\n",
    "\t\treturn (part1*part2)[0,0]\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t    self.X = X\n",
    "\t    self.y = pd.Series(y)\n",
    "\t    self.n = X.shape[0]\n",
    "\t    \n",
    "\t    self.__prior()\n",
    "\t    self.__mean_variance()\n",
    "\n",
    "\tdef predict(self, X_test):\n",
    "\t\tn, m = X_test.shape\n",
    "\t\ty_pre = []\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tx_i = X_test[i,:].reshape((-1,1))\n",
    "\t\t\tP = {}\n",
    "\t\t\tfor c in self.y.unique():\n",
    "\t\t\t    p = self.__calculate_Gaussian_probability(x_i, c)\n",
    "\t\t\t    p = p*self.priors[c]\n",
    "\t\t\t    P[c] = p\n",
    "\t\t\tP = tool.normalizeDict(P)\n",
    "\t\t\ty_pre.append(tool.argmaxDict(P))\n",
    "\n",
    "\t\treturn y_pre\n",
    "\n",
    "\tdef predict_proba(self, X_test):\n",
    "\t\tn, m = X_test.shape\n",
    "\t\ty_pre = []\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tx_i = X_test[i,:].reshape((-1,1))\n",
    "\t\t\tP = {}\n",
    "\t\t\tfor c in self.y.unique():\n",
    "\t\t\t\tp = self.__calculate_Gaussian_probability(x_i, c)\n",
    "\t\t\t\tp = p*self.priors[c]\n",
    "\t\t\t\tP[c] = p\n",
    "\t\t\tP = tool.normalizeDict(P)\n",
    "\t\treturn list(tool.sortDictbyKey(P).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "accs_imp     : 0.9733333333333334\n",
      "accs_imp pca : 0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tool\n",
    "import data\n",
    "\n",
    "# read data\n",
    "dataset_location = \"Iris.csv\"\n",
    "\n",
    "X, y= data.read_csv(dataset_location, shuffle=False)\n",
    "n, m = X.shape\n",
    "print(X.shape)\n",
    "\n",
    "k = 1 # reduced dimension\n",
    "\n",
    "f = n # LEAVE ONE OUT\n",
    "seed = -1\n",
    "# split\n",
    "if seed < 0:\n",
    "    kf = KFold(n_splits = f, shuffle = True)\n",
    "else:\n",
    "    kf = KFold(n_splits = f, random_state = seed, shuffle = True)\n",
    "idxesLists = kf.split(X)\n",
    "splits = []\n",
    "for trainidx, testindx in idxesLists:\n",
    "    splits.append((trainidx, testindx))\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    accs_imp = 0\n",
    "    accs_imp_reduce = 0\n",
    "\n",
    "    for trainidx, testindx in splits:\n",
    "        X_train = X[trainidx,:]\n",
    "        y_train = y[trainidx]\n",
    "        X_test = X[testindx,:]\n",
    "        y_test = y[testindx]\n",
    "\n",
    "        Xt_train = X_train.T\n",
    "        Xt_test = X_test.T\n",
    "        \n",
    "        #1.preprocessing\n",
    "        # remove mean\n",
    "        mean = np.mean(Xt_train,1).reshape(m,-1)\n",
    "        Xt_train = Xt_train - mean\n",
    "        Xt_test = Xt_test - mean\n",
    "        X_train = Xt_train.T\n",
    "        X_test  = Xt_test.T\n",
    "        \n",
    "        # PCA: dimension reduction\n",
    "        D, V = tool.EVD(Xt_train@Xt_train.T)\n",
    "        V = V[:,:k]\n",
    "        Wt_train = V.T@Xt_train\n",
    "\n",
    "        W_train = Wt_train.T\n",
    "        Wt_test = V.T@Xt_test\n",
    "        W_test = Wt_test.T\n",
    "        \n",
    "        #2. TEST\n",
    "        # my implementation: without PCA\n",
    "        clf = NaiveClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pre = clf.predict(X_test)\n",
    "\n",
    "        diff = y_pre - y_test\n",
    "        acc = 1 - np.count_nonzero(diff)/len(y_test)\n",
    "        accs_imp += acc\n",
    "            \n",
    "        # my implementation: with PCA\n",
    "        clf = NaiveClassifier()\n",
    "        clf.fit(W_train, y_train)\n",
    "        y_pre = clf.predict(W_test)\n",
    "\n",
    "        diff = y_pre - y_test\n",
    "        acc = 1 - np.count_nonzero(diff)/len(y_test)\n",
    "        accs_imp_reduce += acc\n",
    "\n",
    "    print('accs_imp     :',accs_imp/f)\n",
    "    print('accs_imp pca :',accs_imp_reduce/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
